{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "This Python notebook is adapted from the supplementary materials for the following forthcoming book chapter:\n",
    "\n",
    "Curini, L., Fahey, RA., \"Chapter 31: Sentiment Analysis and Social Media\", in _The SAGE Handbook of Research Methods in Political Science and International Relations_, Curini, L., Franzese, R. (eds.), London: Sage 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and unpack the data for the Natural Language ToolKit (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\Anna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Anna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('opinion_lexicon')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "\n",
    "First we need to load in the files from the Large Movie Review Dataset. Normally, the dataset is distributed in the form of two directories - which the dataset refers to as \"training\" and \"test\" sets - each containing subdirectories of positive and negative reviews, 12500 in each subdirectory. For the purposes of this workshop, we'll be using a version of the dataset I created that comes in a single file (this will save a lot of time importing the data), but you can find the ordinary version here: [Stanford Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "For this demonstration, we'll treat the full data set (50,000 reviews) as our overall _corpus_. In a later step we'll divide this into \"labelled\" and \"unlabelled\" data, emulating the normal procedure for research projects using this techniques (although in this case it's an artifical division, as in reality the entire dataset is labelled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed value for random numbers, so we all get the same results.\n",
    "np.random.seed(1)      \n",
    " \n",
    "with open('categories.pkl', 'rb') as inputfile:\n",
    "    y = pickle.load(inputfile)\n",
    "    \n",
    "with open('reviews.pkl', 'rb') as inputfile:\n",
    "    X = pickle.load(inputfile)\n",
    "\n",
    "# now X is a list of all the reviews, and y is a list of all their categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's take a look at our data! Use the box below to try out some commands to explore the lists of data contained in *X* (reviews) and *y* (positive/negative).\n",
    "\n",
    "For example, use `len(X)` to find out how many entries are in list *X*; or to see an individual review, try `X[10]` or `X[12700]`. If you reference the element in list *y* with the same number (e.g. `y[10]` or `y[12700]`) you can see whether this review is positive (1) or negative (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wow, I forgot how great this movie was until I stumbled upon it while looking through the garage. It\\'s a kind of strange combination of a bio of Michael Jackson, a collection of musical vignettes, and a story about a super hero fighting to save some little kids. The vignettes are good (especially Speed Demon), but the best part of this movie is the super hero segment, in which Michael Jackson turns into a car, a robot, and finally a spaceship (and it\\'s just as weird as it sounds). Joe Pesci is hilarious, and has enough cool imagery and great music to entertain throughout!<br /><br />The real gem however is the incredible \"Smooth Criminal\" video, which makes the movie worth owning for that part alone!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try some commands here!\n",
    "X[12700]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Pre-Processing\n",
    "\n",
    "This dataset isn't drawn from social media so we don't have to worry about emoji or hashtags in this case. However, a quick look at the data will show that there are some problems - for example, there are HTML tags in the text in some places, and there are also a lot of punctuation or control characters present. Let's get rid of them. \n",
    "\n",
    "We find HTML tags using Regular Expressions - a very powerful and commonly used system for performing advanced searching and lookup within text.\n",
    "\n",
    "While we're at it, we can convert the text to lower-case - we could do this at any stage in the process but might as well do it in pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular expression\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "# Remove HTML tags and convert to lowercase\n",
    "remove_html = re.compile(\"<[^<]+?>\")\n",
    "X = [remove_html.sub(\" \", a_review.lower()) for a_review in X]\n",
    "\n",
    "# Remove all punctuation\n",
    "remove_punctuation = str.maketrans('', '', punctuation)\n",
    "X = [a_review.translate(remove_punctuation) for a_review in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i felt brain dead ill tell you this is the worst film i have ever bought in my ignorance i thought this was the peter jackson film of the same name the performances are so terrible they are laughable the special effects have not stood the test of time and look dire the script promotes that kind of tv movie stare into the middle distance kind of acting the cast look as if they have been taking lessons from joey tribbiani they have one look each and stick to it plus i have never been confused by a movie until i sat down to watch this the is it a dream or no plot is so terrible that frustration sets in within a few minutes avoid like a plague'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into Labelled and Unlabelled sets\n",
    "\n",
    "For the purposes of these demonstrations, we're going to randomly extract 5,000 reviews from the corpus to treat as _labelled_ (i.e. they have been categorised by human coders), with the remaining 45,000 being _unlabelled_. This is done using the `train_test_split` function from Python's `scikit-learn` package.\n",
    "\n",
    "_(In reality, of course, we know the labels of these 45,000 reviews, and later we'll use them to measure the accuracy of the algorithms trained using the labelled set; in an actual research project, however, this data would be entirely unlabelled, so the robustness of the algorithm's predictions would have to be measured entirely using the labelled set.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_labelled, X_unlabelled, y_labelled, y_unlabelled = train_test_split(X, y, \n",
    "                                                                      test_size=0.9)\n",
    "# This will give us 5,000 'labelled' reviews, and 45,000 'unlabelled'\n",
    "\n",
    "len(X_labelled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Dictionary / Lexicon Sentiment Analysis\n",
    "\n",
    "We'll first test some dictionary or lexicon based sentiment analysis approaches on the data. Since these approaches don't have a training step, we'll just see how they perform on the training data itself.\n",
    "\n",
    "For this stage, we're using the *NTLK* package for Python, which includes both the Liu & Hu Sentiment Lexicon and the VADER Sentiment Scoring lexicon and algorithm. This will allow us to see the difference between using the \"na√Øve\" Liu & Hu approach, and the more complex VADER approach.\n",
    "\n",
    "(If you didn't install the lexicon files at the start of this Notebook, you may get an error telling you to do so now. You won't have to do this again for subsequent projects.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"i chose to see this movie because it got a good score here on imdb but a lot of people either have really poor taste or someones been fixing the score  either way it was a real disappointment the movie is exactly as stupid and far fetched as the title would suggest there really is no reason to give a summary of the plot  but here goes it felt like someone had been thinking wouldnt it be cool to make a movie where there were snakes on a plane and then the snakes for some reason would go crazy and start biting and stuff and thats about it the plot is thin and unoriginal the snakes are bad cgi but it makes sense to cut corners on a movie that no one in their right mind will recommend to anyone the acting is poor and all people are unbelievable stereo types  to sum it up its one of the worst movies ive ever seen  stay away\"\n",
      "\n",
      "Negative\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample of Liu & Hu Lexicon output\n",
    "\n",
    "from nltk.sentiment.util import demo_liu_hu_lexicon\n",
    "\n",
    "print('\"{}\"\\n'.format(X[8]))\n",
    "\n",
    "demo_liu_hu_lexicon(X[8], plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liu & Hu accurately classified this review - but note that it identified a number of positive words, despite the review being overwhelmingly negative. This hints at the problem Liu & Hu's lexicon has with many cases; in fact, running the algorithm over a larger data set reveals quite poor accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.7%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual-Neg</th>\n",
       "      <th>Actual-Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predict-Neg</th>\n",
       "      <td>324</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predict-Neu</th>\n",
       "      <td>38</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predict-Pos</th>\n",
       "      <td>139</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Actual-Neg  Actual-Pos\n",
       "Predict-Neg         324          72\n",
       "Predict-Neu          38          34\n",
       "Predict-Pos         139         393"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lexicon_positive = list(opinion_lexicon.positive())\n",
    "lexicon_negative = list(opinion_lexicon.negative())\n",
    "\n",
    "def liu_hu(a_string):\n",
    "    pos_words = 0\n",
    "    neg_words = 0\n",
    "    for word in word_tokenize(a_string):\n",
    "        if word in lexicon_positive:\n",
    "            pos_words += 1\n",
    "        elif word in lexicon_negative:\n",
    "            neg_words += 1\n",
    "    if pos_words > neg_words:\n",
    "        return 'Positive'\n",
    "    elif pos_words < neg_words:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "            \n",
    "pos = [0, 0]\n",
    "neu = [0, 0]\n",
    "neg = [0, 0]\n",
    "\n",
    "for a_review, actual_class in zip(X_labelled[:1000], y_labelled[:1000]):\n",
    "    classification = liu_hu(a_review)\n",
    "    if classification == 'Positive':\n",
    "        pos[actual_class] += 1\n",
    "    elif classification == 'Neutral':\n",
    "        neu[actual_class] += 1\n",
    "    elif classification == 'Negative':\n",
    "        neg[actual_class] += 1\n",
    "\n",
    "accuracy = ((pos[1]+neg[0])/1000)*100\n",
    "print('Accuracy: {}%'.format(accuracy))\n",
    "        \n",
    "result_frame = pd.DataFrame(data=[neg, neu, pos], \n",
    "                            columns=['Actual-Neg', 'Actual-Pos'], \n",
    "                            index=['Predict-Neg', 'Predict-Neu', 'Predict-Pos'])\n",
    "\n",
    "result_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for Liu & Hu Sentiment Lexicon\n",
    "\n",
    "When we ran the Liu & Hu Lexicon on the full data set (which we don't have time to do in this workshop), it managed 69.6% accuracy overall - correctly scoring some 31,329 reviews, but missing the remainder. It was somewhat better at scoring positive reviews (74.3% accurate) than negative reviews (64.9%). Given how simple the Liu & Hu approach is, this is a reasonable score - but it would be questionable to draw any firm research conclusions from a classification approach with such large error margins.\n",
    "\n",
    "Next, let's look at the more complex and context-sensitive VADER sentiment scoring system. Rather than a simple \"Positive\" or \"Negative\", this produces a probability score, with a compound score of -1 being entirely negative and +1 being entirely positive. For the purposes of this example, we'll convert that score into a Positive or Negative rating. Vader can even recognize emoji, making it suitable for social media text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"i chose to see this movie because it got a good score here on imdb but a lot of people either have really poor taste or someones been fixing the score  either way it was a real disappointment the movie is exactly as stupid and far fetched as the title would suggest there really is no reason to give a summary of the plot  but here goes it felt like someone had been thinking wouldnt it be cool to make a movie where there were snakes on a plane and then the snakes for some reason would go crazy and start biting and stuff and thats about it the plot is thin and unoriginal the snakes are bad cgi but it makes sense to cut corners on a movie that no one in their right mind will recommend to anyone the acting is poor and all people are unbelievable stereo types  to sum it up its one of the worst movies ive ever seen  stay away\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.22, 'neu': 0.726, 'pos': 0.054, 'compound': -0.9888}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment import vader\n",
    "\n",
    "vader_sentiment = vader.SentimentIntensityAnalyzer()\n",
    "\n",
    "print('\"{}\"'.format(X[8]))\n",
    "\n",
    "vader_sentiment.polarity_scores(X[8])\n",
    "#gives you the probability distribution of positive and negative, scale from -1(very negative) to 1(very positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.89999999999999%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual-Neg</th>\n",
       "      <th>Actual-Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predict-Neg</th>\n",
       "      <td>258</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predict-Neu</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predict-Pos</th>\n",
       "      <td>243</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Actual-Neg  Actual-Pos\n",
       "Predict-Neg         258          58\n",
       "Predict-Neu           0           0\n",
       "Predict-Pos         243         441"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now to test a larger set using VADER:\n",
    "\n",
    "pos = [0, 0]\n",
    "neu = [0, 0]\n",
    "neg = [0, 0]\n",
    "\n",
    "for a_review, actual_class in zip(X_labelled[:1000], y_labelled[:1000]):\n",
    "    classification = vader_sentiment.polarity_scores(a_review)['compound']\n",
    "    if classification > 0:\n",
    "        pos[actual_class] += 1\n",
    "    elif classification < 0:\n",
    "        neg[actual_class] += 1\n",
    "    elif classification == 0:\n",
    "        neu[actual_class] += 1\n",
    "\n",
    "accuracy = ((pos[1]+neg[0])/1000)*100\n",
    "print('Accuracy: {}%'.format(accuracy))\n",
    "\n",
    "vader_result_frame = pd.DataFrame(data=[neg, neu, pos], \n",
    "                                  columns=['Actual-Neg', 'Actual-Pos'], \n",
    "                                  index=['Predict-Neg', 'Predict-Neu', 'Predict-Pos'])\n",
    "\n",
    "vader_result_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for VADER Sentiment Analysis\n",
    "\n",
    "In tests on the full data set, VADER achieved very similar results to the Liu & Hu Lexicon in this test - correctly classifying 69.6% of the reviews - but it skewed very strongly towards positive classifications. 85.2% of positive reviews were correctly classified, compared to only 54.0% of negative reviews, and there was a high probability of a negative review being misclassified as positive.\n",
    "\n",
    "Since VADER is specifically designed to work with social media texts, it is likely that a data set drawn from Twitter or Facebook would show a more consistent performance for this algorithm.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Algorithms\n",
    "\n",
    "The next set of approaches we will demonstrate are the classification algorithms. For this set of algorithms, we divide the labelled data into _training_ and _test_ sets, train a model using the `training set`, and then test its effectiveness on the `test set`. To ensure reliability, the training and test sets are divided up into multiple folds and the algorithms trained and tested repeatedly to ensure reliability - a process known as _cross-validation_. In this example, we'll also be able to check the performance of the trained algorithm on our \"unlabelled\" data (since in reality we do have labels for that data - we just don't show them to the algorithm during the training stage). In a real-world research project, this final check would not be possible, so the reliability of the algorithm must be determined entirely from its performance on the labelled data set. \n",
    "\n",
    "\n",
    "### Constructing a Document-Term Matrix\n",
    "\n",
    "The first step before doing any classification is to turn our corpus of texts into word vectors, known as a Document-Term Matrix. This represents each document as a binary vector where an element is 1 if that vocabulary word is present, and 0 if it is not present.\n",
    "\n",
    "In order to tidy up our data a little, we will apply a stemmer to the words (reducing them to a stem according to their core meaning) and also remove \"stopwords\" - words like \"the\" and \"a\" which appear very frequently but have little or no relevance to the sentiment of a text. We then \"fit\" the vectorizer to the labelled set, allowing it to learn which vocabulary words are relevant and which to exclude from its vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anna\\Anaconda2\\envs\\Python3.7\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'s\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.5, max_features=500, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<__main__.StemTokenize object at 0x000001F13A2A1898>,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class StemTokenize(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    def __call__(self, articles):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(articles)]\n",
    "\n",
    "dtm_vectorizer = CountVectorizer(tokenizer=StemTokenize(),\n",
    "                                 stop_words = list(stopwords.words('english')),\n",
    "                                 binary = True,\n",
    "                                 max_df = 0.5,  #document frequency > 0.5, remove them cuz they are not useful and unique features\n",
    "                                 max_features = 500) \n",
    "\n",
    "# 'binary' creates boolean (0 or 1) vectors, rather than term frequencies\n",
    "# 'max_df' removes any word appearing in more than that proportion of documents\n",
    "# 'max_features' selects the top N features by frequency\n",
    "\n",
    "dtm_vectorizer.fit(X_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x500 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 228323 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the train and test data to document term matrix\n",
    "X_labelled_dtm = dtm_vectorizer.transform(X_labelled)\n",
    "X_unlabelled_dtm = dtm_vectorizer.transform(X_unlabelled)\n",
    "\n",
    "X_labelled_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X_unlabelled_dtm` and `X_labelled_dtm` now hold large, sparse matrices of data - 500 vocabulary columns for each of the 45,000 unlabelled reviews and 5,000 labelled reviews in the data set. We call this kind of array \"sparse\" because the vast majority of entries in the matrix will be zero; this property of the data allows it to be stored in a computer's memory very efficiently, because we only need to store the locations of the cells which hold data, not of all the zero cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the algorithms\n",
    "\n",
    "Let's begin testing classification algorithms with one of the most simple (but often effective) varieties, Naive Bayes. We'll try out two variations of this algorithm - Multinomial Naive Bayes, and Bernoulli Naive Bayes. The important thing to note here is that the script to train each algorithm is almost identical. This is common across classification algorithms; it's easy to train and test multiple algorithms quickly using pretty much the same lines of Python or R.\n",
    "\n",
    "There are many classification algorithms - from simple methods likeNaive Bayes, to evaluations of regression-based approaches like Support Vector Machine, Random Forest, Neural Networks.\n",
    "\n",
    "In order to carry out the test, we need to sub-divide our labelled data into **training** and **test** sets. The algorithm will be trained using only the training set of data; then the unseen test set will be used to measure its performance. In order to ensure that the algorithm performs consistently on this data set, we use a process called **cross validation** - whereby we divide the labelled data into multiple \"folds\", and train the algorithm on each fold in turn, testing it on the remainder of the data. If it performs consistently strongly across all of the folds, we can say with some confidence that it should give similarly good performance when it's used to predict the classifications of the _unlabelled_ data. This is implemented in _scikit-learn_ using the `cross_val_score` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Multinomial Naive Bayes]\n",
      "Accuracy: 0.824 (Std. Error: +/-0.013) - [0.81883623 0.81943611 0.83253301]\n",
      "\n",
      "[Bernoulli Naive Bayes]\n",
      "Accuracy: 0.813 (Std. Error: +/-0.014) - [0.81703659 0.80383923 0.81932773]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "m_mnb = MultinomialNB()\n",
    "cv_mnb = cross_val_score(m_mnb, X_labelled_dtm, y_labelled, cv=3)\n",
    "print('\\n[Multinomial Naive Bayes]')\n",
    "print(\"Accuracy: {:0.3f} (Std. Error: +/-{:0.03f}) - {}\".format(cv_mnb.mean(),\n",
    "                                                                cv_mnb.std() * 2,\n",
    "                                                                cv_mnb))\n",
    "\n",
    "m_bnb = BernoulliNB()\n",
    "cv_bnb = cross_val_score(m_bnb, X_labelled_dtm, y_labelled, cv=3)\n",
    "print('\\n[Bernoulli Naive Bayes]')\n",
    "print(\"Accuracy: {:0.3f} (Std. Error: +/-{:0.03f}) - {}\".format(cv_bnb.mean(),\n",
    "                                                                cv_bnb.std() * 2,\n",
    "                                                                cv_bnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do cross-validation: cut your data into a few segments and cross train and test on each subsets, in order to make sure that your algorithm fits for every subset, instead of happened to perform well on one specific subset. The more variation in your data (ex. social media), you need to train more percentage of your data; while working on newspaper context, your train size could be at least as 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already we can see that classification algorithms offer a dramatic improvement over the lexicon-based approaches we have been using so far, with both versions of Naive Bayes achieving over 80% accuracy on the test set of data.\n",
    "\n",
    "In the following section we will test multiple other algorithms - Support Vector Classification, Stochastic Gradient Descent, Random Forest, and a Neural Network (Multi-level Perceptron). Random Forest and Neural Network models are particularly complex and may take much longer to train than the other models; in some instances this is worthwhile as the model is much more accurate, but this is not always the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Support Vector Classification]\n",
      "Accuracy: 0.816 (Std. Error: +/-0.009) - [0.80923815 0.81883623 0.81932773]\n",
      "\n",
      "[Stochastic Gradient Descent (hinge, l2)]\n",
      "Accuracy: 0.796 (Std. Error: +/-0.022) - [0.78104379 0.80503899 0.80312125]\n",
      "\n",
      "[Random Forest (10 trees)]\n",
      "Accuracy: 0.731 (Std. Error: +/-0.019) - [0.72525495 0.72405519 0.74489796]\n",
      "\n",
      "[Neural Network (Multi-layer Perceptron)]\n",
      "Accuracy: 0.809 (Std. Error: +/-0.014) - [0.79904019 0.81403719 0.81272509]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "m_svc = SVC(max_iter=-1, gamma='auto')\n",
    "cv_svc = cross_val_score(m_svc, X_labelled_dtm, y_labelled, cv=3)\n",
    "print('\\n[Support Vector Classification]')\n",
    "print(\"Accuracy: {:0.3f} (Std. Error: +/-{:0.03f}) - {}\".format(cv_svc.mean(),\n",
    "                                                                cv_svc.std() * 2,\n",
    "                                                                cv_svc))\n",
    "\n",
    "m_sgd = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1000, tol=1e-3, shuffle=True)\n",
    "cv_sgd = cross_val_score(m_sgd, X_labelled_dtm, y_labelled, cv=3)\n",
    "print('\\n[Stochastic Gradient Descent (hinge, l2)]')\n",
    "print(\"Accuracy: {:0.3f} (Std. Error: +/-{:0.03f}) - {}\".format(cv_sgd.mean(),\n",
    "                                                                cv_sgd.std() * 2,\n",
    "                                                                cv_sgd))\n",
    "\n",
    "m_for10 = RandomForestClassifier(n_estimators=10)\n",
    "cv_for10 = cross_val_score(m_for10, X_labelled_dtm, y_labelled, cv=3)\n",
    "print('\\n[Random Forest (10 trees)]')\n",
    "print(\"Accuracy: {:0.3f} (Std. Error: +/-{:0.03f}) - {}\".format(cv_for10.mean(),\n",
    "                                                                cv_for10.std() * 2,\n",
    "                                                                cv_for10))\n",
    "\n",
    "m_mlp = MLPClassifier(hidden_layer_sizes=(500,250,100))\n",
    "cv_mlp = cross_val_score(m_mlp, X_labelled_dtm, y_labelled, cv=3)\n",
    "print('\\n[Neural Network (Multi-layer Perceptron)]')\n",
    "print(\"Accuracy: {:0.3f} (Std. Error: +/-{:0.03f}) - {}\".format(cv_mlp.mean(),\n",
    "                                                                cv_mlp.std() * 2,\n",
    "                                                                cv_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Algorithm Results\n",
    "\n",
    "There are a number of important points to note in the results from the classification algorithms. Firstly, there is no one-size-fits-all classification algorithm, and more modern, complex approaches like neural networks are not necessarily going to outperform simple algorithms like Naive Bayes. The performance of the different algorithms can vary widely depending on the data being classified; the only way to determine the most appropriate algorithm for your data is to carry out this process of training and testing different types. In this particular instance, the Linear Support Vector Classifier had by far the worst performance of any model tested, but in other sentiment analysis projects it can and will be the top performer.\n",
    "\n",
    "Additionally, many algorithms take a range of \"hyper-parameters\" which are used to fine-tune the model. Discovering the ideal parameters for a specific algorithm and a specific data set is also a process of trial and error - but this, too, can be automated to a large extent. The `scikit-learn` package provides a function called `GridSearchCV` which allows the automated construction and testing of an exhaustive set of combinations of parameter values.  Note that the \"grid\" of hyper-parameters you define (`test_parameters` in the below script) will be different for every algorithm - check the _scikit-learn_ documentation for details of the parameters for each algorithm. This function will also automatically perform cross-validation on the training process, as we did above.\n",
    "\n",
    "The Support Vector Classifier achieved on average 81.9% accuracy in the test above - not as good as the Multinomial Naive Bayes algorithm's score, but the top-ranked among the more complex algorithms with multiple hyper-parameters. Let's now see if we can boost its score by testing different parameters using `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performing hyper-parameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Scores for each iteration of parameters tested:\n",
      "\n",
      "0.822 (+/-0.004) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.672 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.824 (+/-0.022) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.821 (+/-0.004) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.814 (+/-0.016) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.824 (+/-0.019) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.808 (+/-0.016) for {'C': 1, 'kernel': 'linear'}\n",
      "0.808 (+/-0.010) for {'C': 10, 'kernel': 'linear'}\n",
      "0.805 (+/-0.012) for {'C': 100, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "test_parameters = [{'kernel': ['rbf'], \n",
    "                    'gamma': [1e-3, 1e-4],\n",
    "                    'C': [1, 10, 100]},\n",
    "                    {'kernel': ['linear'], \n",
    "                     'C': [1, 10, 100]}]\n",
    "\n",
    "grid_searcher = GridSearchCV(SVC(),            # Model to be tested\n",
    "                             test_parameters,  # Hyper-parameters to test\n",
    "                             scoring='f1',     # Performance score to use\n",
    "                             cv=3,             # Folds for cross-validation\n",
    "                             n_jobs=-1)        # CPU cores to use; -1 is 'all'\n",
    "\n",
    "grid_searcher.fit(X_labelled_dtm, y_labelled)\n",
    "\n",
    "print(\"Best performing hyper-parameters: {}\\n\".format(grid_searcher.best_params_))\n",
    "print(\"Scores for each iteration of parameters tested:\\n\")\n",
    "for mean, std, params in zip(grid_searcher.cv_results_['mean_test_score'], \n",
    "                             grid_searcher.cv_results_['std_test_score'], \n",
    "                             grid_searcher.cv_results_['params']):\n",
    "    print(\"{:0.3f} (+/-{:0.03f}) for {}\".format(mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from Grid Search and Cross Validation\n",
    "\n",
    "We can see here that - with a single exception - all of the Support Vector Classifiers we trained performed well, with the best-performing version only achieving accuracy a few tenths of a percentage point ahead of most of the others (**82.3%**). Crucially, the standard error scores are also low, indicating that there was no outlier in the cross validation test - this means that the classification model is well-adapted to every permutation of the data it was tested on.\n",
    "\n",
    "In a real-world application, the next step would be to take the hyper-parameters discovered through the above process and use them to train a classifier on the complete training set. This trained model could then be used to predict the classifications for entirely unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on scoring classification algorithms\n",
    "\n",
    "When we talk about the \"accuracy\" of a classification algorithm, the most simple way of scoring its performance - the percentage of documents it correctly scored - may actually be misleading. While of course we want this score to be as high as possible, it can also be important to look at the two components which make up that score, known as **recall** and **precision**. \n",
    "\n",
    "**Recall** is a measure of what proportion of instances of a given category the algorithm correctly identified; so for example, if there were 10 instances of the category \"positive\" in the test data set, and the algorithm correctly identified eight of them, we would say that this algorithm has \"recall of 0.8 for the category _positive_\". \n",
    "\n",
    "**Precision** on the other hand is a measure of how many of the times the algorithm identified a category were actually correct, as against how many times were false positives. In the above example, where the algorithm correctly identified eight of the 10 instances of _positive_, perhaps the algorithm also mis-identified four other documents as _positive_ - so eight out of its 12 _positive_ classifications were correct, allowing us to say that it has a \"precision of 0.667 for the category _positive_\".\n",
    "\n",
    "The aggregate of the recall and precision scores for a category is known as the **f1 score**, and the average of the f1 scores for all the categories is a resonable rough measurement of the performance of the algorithm. However, before using the algorithm for any serious analysis work, it is advisable to take a look at the precision and recall scores for individual categories - you may find that a category you are planning to use in your analysis actually has very high rate of false-positive or false-negative identifications, which could cause serious problems for your results.\n",
    "\n",
    "In scikit-learn, we can look at this data using the `classification_report` function, whose use is shown below. First we `predict` the classifications of our held-out test set data (`X_unlabelled`, 45,000 reviews) using the model created in the previous step; then the `classification_report` function compares the predictions to the actual classifications stored in `y_unlabelled`. Note that 0 and 1 here correspond to negative and positive classifications respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.81      0.82     22526\n",
      "           1       0.82      0.85      0.83     22474\n",
      "\n",
      "   micro avg       0.83      0.83      0.83     45000\n",
      "   macro avg       0.83      0.83      0.83     45000\n",
      "weighted avg       0.83      0.83      0.83     45000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual-Neg</th>\n",
       "      <th>Actual-Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predict-Neg</th>\n",
       "      <td>18242</td>\n",
       "      <td>3458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predict-Pos</th>\n",
       "      <td>4284</td>\n",
       "      <td>19016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Actual-Neg  Actual-Pos\n",
       "Predict-Neg       18242        3458\n",
       "Predict-Pos        4284       19016"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.828    (Positive: 0.846  Negative: 0.810)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_predictions = grid_searcher.predict(X_unlabelled_dtm)    \n",
    "# Predictions on the test set using the best model found by GridSearchCV above\n",
    "\n",
    "print(classification_report(y_unlabelled, X_predictions))\n",
    "\n",
    "pos = [0, 0]\n",
    "neg = [0, 0]\n",
    "\n",
    "for predicted_class, actual_class in zip(X_predictions, y_unlabelled):\n",
    "    if predicted_class == 1:\n",
    "        pos[actual_class] += 1\n",
    "    elif predicted_class == 0:\n",
    "        neg[actual_class] += 1\n",
    "\n",
    "result_frame = pd.DataFrame(data=[neg, pos], \n",
    "                            columns=['Actual-Neg', 'Actual-Pos'], \n",
    "                            index=['Predict-Neg', 'Predict-Pos'])\n",
    "\n",
    "display(result_frame)\n",
    "\n",
    "accuracy_score = (pos[1] + neg[0]) / (sum(pos) + sum(neg))\n",
    "pos_accuracy = pos[1] / (pos[1] + neg[1])\n",
    "neg_accuracy = neg[0] / (neg[0] + pos[0])\n",
    "\n",
    "print(\"Accuracy: {:0.3f}    (Positive: {:0.3f}  Negative: {:0.3f})\".format(accuracy_score,\n",
    "                                                                           pos_accuracy,\n",
    "                                                                           neg_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see subtle differences in how the model performs on positive and negative reviews. For negative reviews, the _precision_ is slightly higher (0.84) than the _recall_ (0.81) - both are good scores but this means that the model tends to miss actual instances of negative reviews a little more often than it mis-classifies a review as negative. None of these scores are low enough to indicate a major problem with the model - while there is room for improvement here, accuracy scores in these ranges are generally seen as being usable for research purposes.\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Algorithms\n",
    "\n",
    "These algorithms predict the overall distribution of sentiment within the data set rather than classifying individual documents. In this case, the data set is divided perfectly in half, so an aggregate algorithm would hopefully come close to recognising that 50:50 split.\n",
    "\n",
    "There are two aggregate algorithms available to researchers - ReadMe and iSA. While packages for both algorithms are available for R, only iSA has been made available on Python (through the [PyiSAX package](https://github.com/robfahey/PyiSA) - I developed this so if you find a bug, please let me know), so we will use that algorithm for this demonstration.\n",
    "\n",
    "Aggregate algorithms take exactly the same Document-Term Matrix as input as classification algorithms do, so we can re-use our `X_labelled` and `X_unlabelled` matrices here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate Algorithms are similar to classification allgorithms in many ways (they need training data and function on pattern recognition), but different in one crucial way - they do not classify individual documents. AA tend to give more accurate results with a much smaller amount of training data. And AA is good at handling corpuses with a lot of \"off-topic\" data. But you can't explore individual data with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Commencing iSA run (verbose mode enabled)...\n",
      "Feature Space: 125 features X 50000 documents (5000 training, 45000 test)\n",
      "Augmenting dataset using 25 splits...\n",
      "Training iSA for 2 categories.\n",
      "Absolute determinant of (P'*P): 2.932843846237651e-07\n",
      "Bootstrapping... (1000 passes)\n",
      "[DONE]   Execution time: 269.087 seconds\n",
      "\n",
      "Results:\n",
      "            Estimate  Std. Error    z value       Pr(>|z|)\n",
      "categories                                                \n",
      "0           0.509353    0.016569  30.741385  1.594043e-207\n",
      "1           0.490647    0.016569  29.612449  1.033172e-192\n"
     ]
    }
   ],
   "source": [
    "from pyisax import PyiSA\n",
    "\n",
    "X_train_isa = PyiSA.prep_data(X_labelled_dtm)\n",
    "X_test_isa = PyiSA.prep_data(X_unlabelled_dtm)\n",
    "\n",
    "isa_model = PyiSA(verbose=True)\n",
    "isa_model.fit(X_train_isa, X_test_isa, y_labelled)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(isa_model.best_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iSA Results\n",
    "\n",
    "Using the same training and test set (5,000 training reviews, 45,000 test reviews) as was used in the classification examples above, the iSA algorithm predicts the actual distribution of opinion within the reviews (50:50) to within a few percentage points (51.4 to 48.6). It also estimated a standard error for both categories of 1%. \n",
    "\n",
    "Note that while the classification algorithm also achieved good aggregate results, this was due to mis-classifying similar numbers of reviews from both categories - in a situation where there were more than two categories (e.g. \"off-topic\" texts were included) or where the final distribution of reviews was uneven, the aggregate iSA algorithm would be more likely to retain its accuracy than the classification algorithm. Aggregate algorithms are also better equipped to handle smaller amounts of training data - if we re-run this test with just a few hundred training reviews, the accuracy of the classifier would drop off sharply while the aggregate algorithm ought to remain more consistent.\n",
    "\n",
    "As a final point, note that iSA completed its operations in just over two minutes - when the amount of time taken to carry out grid-search and cross validation is taken into account, the training and execution of the classification algorithm can take several hours on a normal computer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
